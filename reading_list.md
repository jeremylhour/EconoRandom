# Liste de références bibliographiques utiles pour les data scientists

Cette liste a pour vocation d'offrir des points d'entrée dans les sous-domaines en question. Elle regroupe principalement les manuels qui font référence, mais aussi des documents de recherche quand cela est approprié. 

Auteur : Jérémy L'Hour, <jeremy.lhour@insee.fr>.

## Python
- 'Automate the boring stuff with python' (2nd Edition, 2019) de Sweigart

Livre d'introduction à python, très bien expliqué. Permet d'apprendre les bases, mais aussi des choses plus originales comme traiter des images, gérer la lecture de fichier, gérer les tableurs, scraper des données sur internet, envoyer des e-mails, programmer des tâches à des heures précises, etc. [Accessible gratuitement en ligne](https://automatetheboringstuff.com/)

- 'Beyond the basic stuff with python' (2020) de Sweigart

Même auteur que le précédent, mais plus avancé. Avec notamment des chapitres sur la programmation orientée objet, l'utilisation de git, les conventions d'écriture du code, l'analyse de la complexité des algorithmes.

## Apprentissage statistique / automatique
- 'The Elements of Statistical Learning' (2nd Edition, 2009) de Tibshirani et Hastie

Manuel de référence pour toutes les approches de machine learning "traditionnelles". [Accessible gratuitement en ligne](https://web.stanford.edu/~hastie/ElemStatLearn/)

- 'Pattern Recognition and Machine Learning' (2006) de Bishop

## Apprentissage profond
- 'Deep Learning' (2016) de Goodfellow, Bengio et Courville

Livre de référence sur l'approche Deep Learning, couvre beaucoup de sujets classiques mais aussi des sujets avancés et l'état de la recherche. [Accessible gratuitement en ligne](https://www.deeplearningbook.org/)

- 'A recipe for training neural network' (blog post, 2019) de Karpathy

Mode d'emploi pour s'assurer de faire diminuer la perte, une complication à la fois / debugger ses modèles. [Accessible en ligne](http://karpathy.github.io/2019/04/25/recipe/)

- 'Neural networks, manifolds and topology' (blog post, 2014) d'Olah

Réflexions utiles sur le fonctionnement des réseaux de neurones "en théorie". [Accessible en ligne](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)

## Traitement du langage naturel
- 'Speech and Language Processing' (3rd Edition, 2021?) de Jurasfky et Martin

Manuel de référence sur le NLP. Couvre beaucoup de tâches importantes en analyse textuelle. [Version préliminaire accessible gratuitement en ligne](https://web.stanford.edu/~jurafsky/slp3/)

- 'Stanford CS 224N : Natural Language Processing with Deep Learning' (cours)
 
Ce cours couvre beaucoup de thèmes intéressants en NLP et propose des devoirs-maison à réaliser en python. [Cours accessible en ligne](http://web.stanford.edu/class/cs224n/)

- 'Deep learning for NLP: best practices' (blog post, 2017) de Ruder

[Accessible en ligne](https://ruder.io/deep-learning-nlp-best-practices/)

## Statistiques mathématiques
- 'All of Statistics' (2004) de Wasserman

Référence "standard" pour les concepts classiques de statistique.

## Econométrie / inférence causale
- 'Machine Learning Methods Economists Should Know About' (2019) d'Athey et Imbens

Introduction à l'utilité des méthodes de ML en économétrie/économie empirique. S'adresse particulièrement aux économistes empiriques peu au courant des méthodes de ML. Mais offre également une perspective sur l'utilisation des méthodes de ML pour des tâches classiques en économétrie. [Accessible sur ArXiv](https://arxiv.org/abs/1903.10075)

- 'Machine Learning for Econometrics' (2021?) de L'Hour et Gaillac

Polycopié du cours de 3A ENSAE, développe les thèmes du papier précédent. [Polycopié accessible en ligne](https://sites.google.com/site/jeremylhour/courses)

- 'Econometric Analysis of Cross-Section and Panel Data' (2010) de Wooldridge

Référence classique pour les concepts / modèles standards en économétrie.

- 'Causal Inference for Statistics, Social, and Biomedical Sciences' (2015) d'Imbens et Rubin

Référence pour l'inférence causale.

- 'A first course un experimental design' (2020) d'Owen

Poycopié du cours correspondant à Stanford. Couvre notamment les A/B tests, les bandits, etc. [Notes disponibles en ligne](https://statweb.stanford.edu/~owen/courses/363/)

## Conception de projets machine learning / data
- 'Building Machine Learning Powered Applications' (2020) d'Ameisen
- 'Designing Data–Intensive Applications' (2016) de Kleppmann

## Références générales
- 'Calling Bullshit' (2020) de Bergstrom et West
- 'The Signal and The Noise' (2012) de Silver
- 'Superforecasting' (2015) de Gardner et Tetlock
- 'You are not so smart' (2011) de McRaney
- 'You are now less dumb' (2013) de McRaney
- 'Range' (2019) d'Epstein
